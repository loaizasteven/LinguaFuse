{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d87baea",
   "metadata": {},
   "source": [
    "# LinguaFuse Fine-Tuning Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the LinguaFuse framework to load and process a sample dataset, and fine-tune a transformer model based on different scopes (Local, AWS, AML)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac35af",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "Ensure you have installed the project requirements and import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e52ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# %pip install -r ../requirements.txt\n",
    "\n",
    "# Imports\n",
    "from pathlib import Path\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to sys.path to resolve imports\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "sys.path.append(str(root_dir / \"libs\"))\n",
    "\n",
    "from linguafuse.cloud import Scope\n",
    "from linguafuse.framework import (\n",
    "    FineTuneOrchestration,\n",
    "    LocalDataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5f632",
   "metadata": {},
   "source": [
    "## 2. Load and Process Dataset\n",
    "\n",
    "Use Local scope to load the sample CSV, then process into a `ProcessedDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2a238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting locally with asset: path=PosixPath('/Users/steven/git/LinguaFuse/tests/example_data.csv') <class 'linguafuse.framework.LocalDataArguments'>\n",
      "Dataset columns: ['label', 'encoded_label', 'text']\n",
      "Number of examples: 10\n"
     ]
    }
   ],
   "source": [
    "# Define sample data path\n",
    "sample_path = root_dir / 'tests' / 'example_data.csv'\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set up orchestration for local dataset\n",
    "local_args = LocalDataArguments(path=sample_path)\n",
    "orl = FineTuneOrchestration(data_args=local_args, scope=Scope.LOCAL, tokenizer=tokenizer)\n",
    "\n",
    "# Process dataset\n",
    "orl._create_dataset()\n",
    "print(f\"Dataset columns: {orl.dataset.data.columns.tolist()}\")\n",
    "print(f\"Number of examples: {len(orl.dataset.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cde1b4",
   "metadata": {},
   "source": [
    "## 3. Load Transformer Model\n",
    "\n",
    "Load the transformer model with the correct `num_labels` inferred from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aeeeb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting locally with asset: path=PosixPath('/Users/steven/git/LinguaFuse/tests/example_data.csv') <class 'linguafuse.framework.LocalDataArguments'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config num_labels: 3\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = orl.load_model('bert-base-uncased')\n",
    "print(f\"Model config num_labels: {orl.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b552a1",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "\n",
    "- You can extend this notebook to perform training loops using the loaded model and data loaders.\n",
    "- Experiment with AWS or AML scopes by providing `AwsDataArguments` or `AmlDataArguments` and appropriate credentials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
