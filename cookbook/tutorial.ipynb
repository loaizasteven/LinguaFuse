{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d87baea",
   "metadata": {},
   "source": [
    "# LinguaFuse Fine-Tuning Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the LinguaFuse framework to load and process a sample dataset, and fine-tune a transformer model based on different scopes (Local, AWS, AML)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac35af",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "Ensure you have installed the project requirements and import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e52ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/Library/Caches/pypoetry/virtualenvs/linguafuse-OkTp6vEW-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (run once)\n",
    "# %pip install -r ../requirements.txt\n",
    "\n",
    "# Imports\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path to resolve imports\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "sys.path.append(str(root_dir / \"libs\"))\n",
    "\n",
    "from linguafuse.cloud import Scope\n",
    "from linguafuse.trainer import TrainerArguments\n",
    "from linguafuse.framework import (\n",
    "    FineTuneOrchestration,\n",
    "    LocalDataArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5f632",
   "metadata": {},
   "source": [
    "## 2. Load and Process Dataset\n",
    "\n",
    "Use Local scope to load the sample CSV, then process into a `ProcessedDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2a238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting locally with asset: path=PosixPath('/Users/steven/git/LinguaFuse/tests/example_data.csv') <class 'linguafuse.framework.LocalDataArguments'>\n",
      "Hint: Expecting 'data' to be a tuple of (text, labels)\n",
      "Unique labels: [0 1 2]\n",
      "Counts: [4 5 1]\n",
      "Filtered labels: [0 1]\n",
      "Filtered indices: [ True  True  True  True  True False  True  True  True  True]\n",
      "Represented data: (array(['This product exceeds my expectations! Great quality.',\n",
      "       'Poor customer service and long delivery times.',\n",
      "       'The interface is intuitive and user-friendly.',\n",
      "       'Broke after two weeks of use. Disappointed.',\n",
      "       'Excellent value for money and fast shipping.',\n",
      "       'Love the design and functionality!',\n",
      "       'Missing crucial features and documentation.',\n",
      "       'Quick installation and smooth performance.',\n",
      "       'Not worth the price. Save your money.'], dtype=object), array([1, 0, 1, 0, 1, 1, 0, 1, 0]))\n",
      "Sampling 0.2 of the data\n",
      "Dataset columns: ['label', 'encoded_label', 'text']\n",
      "Number of examples: 10\n"
     ]
    }
   ],
   "source": [
    "# Define sample data path\n",
    "sample_path = root_dir / 'tests' / 'example_data.csv'\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Set up orchestration for local dataset\n",
    "local_args = LocalDataArguments(path=sample_path)\n",
    "orl = FineTuneOrchestration(\n",
    "    data_args=local_args, \n",
    "    scope=Scope.LOCAL, \n",
    "    tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "# Process dataset\n",
    "orl._create_dataset()\n",
    "print(f\"Dataset columns: {orl.processed_dataset.data.columns.tolist()}\")\n",
    "print(f\"Number of examples: {len(orl.processed_dataset.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cde1b4",
   "metadata": {},
   "source": [
    "## 3. Load Transformer Model\n",
    "\n",
    "Load the transformer model with the correct `num_labels` inferred from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aeeeb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config num_labels: 3\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = orl.load_model('bert-base-uncased')\n",
    "print(f\"Model config num_labels: {orl.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b552a1",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "\n",
    "- You can extend this notebook to perform training loops using the loaded model and data loaders.\n",
    "- Experiment with AWS or AML scopes by providing `AwsDataArguments` or `AmlDataArguments` and appropriate credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61caff6",
   "metadata": {},
   "source": [
    "## 5. Fine Tune Model\n",
    "\n",
    "In this section, we will fine-tune the loaded transformer model using the LinguaFuse framework.  \n",
    "You will define training arguments such as batch size, learning rate, number of epochs, and optimizer.  \n",
    "The `orl.train()` method will handle the training loop, evaluation, and model saving automatically.\n",
    "\n",
    "Make sure your dataset and tokenizer are correctly set up before starting the fine-tuning process.  \n",
    "You can further customize the training by adjusting the parameters in `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51167c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: <torch.utils.data.dataloader.DataLoader object at 0x168879a90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example {'input_ids': tensor([[ 101, 3532, 8013,  ...,    0,    0,    0],\n",
      "        [ 101, 3631, 2044,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 8278,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2023, 4031,  ...,    0,    0,    0],\n",
      "        [ 101, 2293, 1996,  ...,    0,    0,    0],\n",
      "        [ 101, 2009, 2573,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 1, 1, 0, 1, 1, 2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:linguafuse.trainer:Starting epoch 1/1\n",
      "INFO:linguafuse.trainer:Starting step iteration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 1/1 [00:16<00:00, 16.27s/it, loss=loss.item():.4f]\n",
      "INFO:linguafuse.trainer:Epoch 1 completed with Training loss: 1.1719036102294922\n"
     ]
    }
   ],
   "source": [
    "# Define LinguaFuse training arguments\n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "OPT = AdamW(params=model.parameters(), lr=LR)\n",
    "WARM_UP = 10\n",
    "STEPS = orl.processed_dataset.get_training_steps(epochs=EPOCHS)\n",
    "\n",
    "lf_training_args = TrainerArguments(\n",
    "    name=\"linguafuse_trainer\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    epochs=EPOCHS,\n",
    "    save_model=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    training_data=None, # will be set in orl.train() implicitly\n",
    "    validation_data=None,\n",
    "    optimizer=OPT,\n",
    "    scheduler=get_linear_schedule_with_warmup(optimizer=OPT, num_warmup_steps=WARM_UP, num_training_steps=STEPS),\n",
    ")\n",
    "\n",
    "# Fine-tune the model using LinguaFuse framework\n",
    "orl.train(trainer_args=lf_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be917027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguafuse-OkTp6vEW-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
